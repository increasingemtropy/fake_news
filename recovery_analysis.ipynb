{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reCOVery analysis\n",
    "\n",
    "Here we do some analysis and modelling with the reCOVery dataset https://github.com/apurvamulay/ReCOVery, used under Attribution-NonCommercial-ShareAlike 4.0 International licence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies:\n",
    " - pandas\n",
    " - numpy\n",
    " - url library\n",
    " - string io\n",
    " - re (regular expression)\n",
    " - spacy\n",
    " - matplotlib\n",
    " - seaborn\n",
    "\n",
    "We also initialise the spacy nlp object in English here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "Read data from URL, load into dataframe and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://raw.githubusercontent.com/apurvamulay/ReCOVery/master/dataset/recovery-news-data.csv'\n",
    "\n",
    "response = urllib.request.urlopen(URL)\n",
    "data = response.read()\n",
    "text = data.decode('utf-8')\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.read_csv(StringIO(text), sep=',') # index_col=[0, 1, 2, 3\n",
    "\n",
    "#uncomment this lines to read from local source for offline work\n",
    "#df = pd.read_csv('FakeCovid_July2020.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Set all body text to lower case, remove troublesome characters and remove repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x):\n",
    "    #all lower case and remove slashes and underscores\n",
    "    x = str(x).lower().replace('\\ ', '').replace('_', ' ')\n",
    "    # remove repeated characters\n",
    "    x = re.sub(r'([a-z])\\1{3,}', r'\\1\\1', x)\n",
    "    return x\n",
    "\n",
    "df['body_text'] = df['body_text'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing and initial analysis\n",
    "\n",
    "Create a corpus of covid news articles from the body text of each entry in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_arts = [nlp(art) for art in df['body_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to locate entities matching a given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_occurences(doc,tag = 'ORG'):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed list of articles\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "    \n",
    "    found_entities = Counter()\n",
    "    for art in doc:\n",
    "        for ent in art.ents:\n",
    "            if ent.label_ == tag:\n",
    "                found_entities[ent.lemma_] += 1\n",
    "              \n",
    "    return found_entities.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print lists of the top 20 most mentioned organisations, people and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_entity_occurences(covid_arts,'ORG')[:20])\n",
    "print(find_entity_occurences(covid_arts,'PERSON')[:20])\n",
    "print(find_entity_occurences(covid_arts,'GPE')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of common entities\n",
    "We can either crab the most common entities as identified by Spacy, or we can define four ourselves a list of entities that we think are important. These operate essentially like filters to identify articles/documents that are relevant or otherwise germane to our investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_groups = [item[0] for item in find_entity_occurences(covid_arts,'ORG')[:20]]\n",
    "common_locations = [item[0] for item in find_entity_occurences(covid_arts,'GPE')[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_groups = [\n",
    "    'afp',\n",
    "    'cdc',\n",
    "    'world health organisation',\n",
    "    'who',\n",
    "    'cnn',\n",
    "    'fox news',\n",
    "    'new york times',\n",
    "    'trump administration',\n",
    "    'the white house',\n",
    "    'congress',\n",
    "    'senate'\n",
    "]\n",
    "\n",
    "common_locations = [\n",
    "    'india',\n",
    "    'england', \n",
    "    'united states', \n",
    "    'us', \n",
    "    'uk', \n",
    "    'china',\n",
    "    'italy',\n",
    "    'spain',\n",
    "    'canada',\n",
    "    'europe',\n",
    "    'asia',\n",
    "    'america'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the co-incidence of various entities within the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_location_dict = defaultdict(Counter)\n",
    "\n",
    "for art in covid_arts:\n",
    "    \n",
    "    group_candidates = []\n",
    "    location_candidates = []\n",
    "    \n",
    "    for ent in art.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "            group_candidates.append(ent.lemma_)\n",
    "        if ent.label_ == 'GPE':\n",
    "            location_candidates.append(ent.lemma_)\n",
    "            \n",
    "    groups = []\n",
    "    locations = []\n",
    "    \n",
    "    for ent in group_candidates:\n",
    "        if ent in common_groups and ent not in groups:\n",
    "            groups.append(ent)\n",
    "    for loc in location_candidates:\n",
    "        if loc in common_locations and loc not in locations:\n",
    "            locations.append(loc)\n",
    "            \n",
    "    for found_entity in groups:\n",
    "        for found_location in locations:\n",
    "            group_location_dict[found_entity][found_location] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dictionary into a pandas DataFrame and fill NaN values with zeroes\n",
    "group_location_df = pd.DataFrame.from_dict(dict(group_location_dict), dtype=int)\n",
    "group_location_full_df = group_location_df.fillna(value=0).astype(int)\n",
    "# Show DF to console\n",
    "group_location_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn can transform a DataFrame directly into a figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "hmap = sns.heatmap(group_location_full_df, ax=ax, annot=True, fmt='d', cmap='YlGnBu', cbar=False)\n",
    "\n",
    "# Add features using the under the hood plt interface\n",
    "plt.title('Global distribution of groups appearing in fake news')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('fake_news.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Classification models\n",
    "\n",
    "Here we try some typical classification models and use the 'reliability' column to do supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support-Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import sklearn tools for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Td-Idf vectoriser.\n",
    "\n",
    "Our input vector will be the vectorised body-text of each article and the output vector is the 'reliability' (0 for unreliable, 1 for reliable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X = df['body_text']\n",
    "y = df['reliability']\n",
    "\n",
    "X = tfidf.fit_transform(X)\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, train then test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logsistic regression\n",
    "TBC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k means / knn clustering\n",
    "TBC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling\n",
    "\n",
    "Here we use the gensim library to perform topic modelling. We can save and load previously trained models using the following code snippet:\n",
    "\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "        lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "    loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "    os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gensim NLP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim import models\n",
    "#from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump all text into a single string and separate out articles with new lines (we just demo with titles for now...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df.title.str.cat(sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text using SpaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out stop-words that are not necessarily instructive for gaining deeper insight.\n",
    "That is, it's clear that news articles about coronavirus will mention coronavirus a lot, so we wish to ignore this for now so it does not dominate the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u\" \",u'pandemic',u'coronavirus',u'novel_coronavirus',u'novel',u'covid-19']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to an article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if we find a new line, move onto the next article\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some bigrams might occur like \"New York\" so let's handle those automatically here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]\n",
    "dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bag-of-words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorise the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus_tfidf, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model\n",
    "\n",
    "N.B. According to Gensim documentation, LDA works with BOW vectorised corpus, but it will accept a Tf-Idf vectorised corpus anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdpmodel = models.HdpModel(corpus=corpus_tfidf, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdpmodel.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
