{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling on fake_covid data\n",
    "\n",
    "We use Gensim to perform some basic topic modelling on text from the fake_covid data set from https://github.com/Gautamshahi/FakeCovid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies:\n",
    " - pandas\n",
    " - numpy\n",
    " - url library\n",
    " - string io\n",
    " - re (regular expression)\n",
    " - gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preview data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://raw.githubusercontent.com/Gautamshahi/FakeCovid/master/data/FakeCovid_July2020.csv'\n",
    "\n",
    "response = urllib.request.urlopen(URL)\n",
    "data = response.read()\n",
    "text = data.decode('utf-8')\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.read_csv(StringIO(text), sep=',') # index_col=[0, 1, 2, 3\n",
    "\n",
    "#uncomment this lines to read from local source for offline work\n",
    "#df = pd.read_csv('FakeCovid_July2020.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up some of the abbreviations in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lang\"]= df[\"lang\"].replace('en', \"English\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('es', \"Spanish\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('fr', \"French\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('pt', \"Portuguese\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('tr', \"Turkish\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('hi', \"Hindi\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('zh-tw', \"Chinese\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('hr', \"Croatian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('te', \"Telugu\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('it', \"Italian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('mk', \"Macedonian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('de', \"German\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ar', \"Arabic\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('id', \"Indonesian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ml', \"Malayalam\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ja', \"Japanese\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ta', \"Tamil\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ko', \"Korean\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('lt', \"Lithuanian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('pl', \"Polish\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('da', \"Danish\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('mr', \"Marathi\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('tl', \"Tagalog\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ru', \"Russian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('nl', \"Dutch\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('fa', \"Persian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('bn', \"Bengali\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('el', \"Greek\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('lv', \"Latvian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('gu', \"Gujarati\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('et', \"Estonian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('uk', \"Ukrainian\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ur', \"Urdu\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('th', \"Thai\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('ca', \"Catalan\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('vi', \"Vietnamese\")\n",
    "df[\"lang\"]= df[\"lang\"].replace('fi', \"Finnish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just focusing on explicitly fake news in English for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[df['lang'] == 'English'].copy()\n",
    "df2['class']= df2['class'].replace('FALSE', 'False')\n",
    "df2['class']= df2['class'].replace('false', 'False')\n",
    "df3 = df2.loc[df2['class'] == 'False'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text in the content_text column\n",
    "Make it all lower case, remove numbers and remove some special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(x):\n",
    "    #all lower case and remove slashes and underscores\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', ' ').replace('/ ','')\n",
    "    # remove repeated characters\n",
    "    x = re.sub(r'([a-z])\\1{3,}', r'\\1\\1', x)\n",
    "    return x\n",
    "\n",
    "df3['title'] = df3['title'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump all text into a single object for analysis (just titles for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df3.title.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text using SpaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out stop-words that are not necessarily instructive for gaining deeper insight.\n",
    "That is, it's clear that fake news articles about coronavirus will mention coronavirus a lot, so we wish to ignore this for now so it does not dominate the BOW vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [u\" \",u'claim',u'people',u'show',u'kill',u'pandemic',u'coronavirus',u'novel_coronavirus',u'novel',u'covid-19']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some bigrams might occur like \"New York\" so let's handle those automatically here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = Counter()\n",
    "for chunk in doc.noun_chunks:\n",
    "    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitrarily selected threshold\n",
    "        keywords[chunk.lemma_] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
