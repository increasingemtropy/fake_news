{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy analysis on fake_covid data\n",
    "\n",
    "We use SpaCy to do some rudimentary analysisof text from the fake_covid data set from https://github.com/Gautamshahi/FakeCovid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies:\n",
    " - pandas\n",
    " - numpy\n",
    " - url library\n",
    " - string io\n",
    " - re (regular expression)\n",
    " - spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from URL, load into dataframe and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://github.com/apurvamulay/ReCOVery/blob/master/dataset/recovery-news-data.csv'\n",
    "\n",
    "response = urllib.request.urlopen(URL)\n",
    "data = response.read()\n",
    "text = data.decode('utf-8')\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.read_csv(StringIO(text), sep=',',header=None) # index_col=[0, 1, 2, 3\n",
    "\n",
    "#uncomment this lines to read from local source for offline work\n",
    "#df = pd.read_csv('FakeCovid_July2020.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter all english language documents then clean the content text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[df['lang'] == 'en'].copy()\n",
    "def text_clean(x):\n",
    "    #all lower case and remove slashes and underscores\n",
    "    x = str(x).lower().replace('\\ ', '').replace('_', ' ')\n",
    "    # remove repeated characters\n",
    "    x = re.sub(r'([a-z])\\1{3,}', r'\\1\\1', x)\n",
    "    return x\n",
    "\n",
    "df2['content_text'] = df2['content_text'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process texts with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_arts = [nlp(art) for art in df2['content_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to locate entities matching a given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_occurences(doc,tag = 'ORG'):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed list of articles\n",
    "    :return: list of tuples in form\n",
    "        [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266)]\n",
    "    \"\"\"\n",
    "    \n",
    "    found_entities = Counter()\n",
    "    for art in doc:\n",
    "        for ent in art.ents:\n",
    "            if ent.label_ == tag:\n",
    "                found_entities[ent.lemma_] += 1\n",
    "              \n",
    "    return found_entities.most_common()\n",
    "\n",
    "print(find_entity_occurences(covid_arts,'ORG')[:20])\n",
    "print(find_entity_occurences(covid_arts,'GPE')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of common entities\n",
    "We can either crab the most common entities as identified by Spacy, or we can define four ourselves a list of entities that we think are important. These operate essentially like filters to identify articles/documents that are relevant or otherwise germane to our investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_groups = [item[0] for item in find_entity_occurences(covid_arts,'ORG')[:20]]\n",
    "common_locations = [item[0] for item in find_entity_occurences(covid_arts,'GPE')[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_groups = [\n",
    "    'afp',\n",
    "    'cdc',\n",
    "    'world health organisation',\n",
    "    'who',\n",
    "    'cnn',\n",
    "    'fox news',\n",
    "    'new york times',\n",
    "    'trump administration',\n",
    "    'the white house',\n",
    "    'congress',\n",
    "    'senate'\n",
    "]\n",
    "\n",
    "common_locations = [\n",
    "    'india',\n",
    "    'england', \n",
    "    'united states', \n",
    "    'us', \n",
    "    'uk', \n",
    "    'china',\n",
    "    'italy',\n",
    "    'spain',\n",
    "    'canada',\n",
    "    'europe',\n",
    "    'asia',\n",
    "    'america'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the co-incidence of various entities within the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_location_dict = defaultdict(Counter)\n",
    "\n",
    "# go through each article in the corpus\n",
    "for art in covid_arts:\n",
    "    \n",
    "    group_candidates = []\n",
    "    location_candidates = []\n",
    "    \n",
    "    # get all entities of potential interest in this article\n",
    "    for ent in art.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "            group_candidates.append(ent.lemma_)\n",
    "        if ent.label_ == 'GPE':\n",
    "            location_candidates.append(ent.lemma_)\n",
    "            \n",
    "    groups = []\n",
    "    locations = []\n",
    "    \n",
    "    # add entities to the lists if they are also on our interest lists\n",
    "    # be careful not to count each entity multiple times!!\n",
    "    for ent in group_candidates:\n",
    "        if ent in common_groups and ent not in groups:\n",
    "            groups.append(ent)\n",
    "    for loc in location_candidates:\n",
    "        if loc in common_locations and loc not in locations:\n",
    "            locations.append(loc)\n",
    "    \n",
    "    # using the entities found above, count the co-incidence in this article        \n",
    "    for found_entity in groups:\n",
    "        for found_location in locations:\n",
    "            group_location_dict[found_entity][found_location] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dictionary into a pandas DataFrame and fill NaN values with zeroes\n",
    "group_location_df = pd.DataFrame.from_dict(dict(group_location_dict), dtype=int)\n",
    "group_location_full_df = group_location_df.fillna(value=0).astype(int)\n",
    "# Show DF to console\n",
    "group_location_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Using the seaborn library, we can make a slightly more elegant (and exportable) figure than merely displaying the dataframe we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn can transform a DataFrame directly into a figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "hmap = sns.heatmap(group_location_full_df, ax=ax, annot=True, fmt='d', cmap='YlGnBu', cbar=False)\n",
    "\n",
    "# Add features using the under the hood plt interface\n",
    "plt.title('Global distribution of groups appearing in fake news')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more fancy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also mask all the zero figures using features of the DataFrame\n",
    "heat_mask = group_location_df.isnull()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "\n",
    "hmap = sns.heatmap(group_location_full_df, ax=ax, annot=True, fmt='d', cmap='YlGnBu', cbar=False, mask=heat_mask)\n",
    "\n",
    "# Add features using the under the hood plt interface\n",
    "sns.axes_style('white')\n",
    "plt.title('Global distribution of groups appearing in fake news')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('fake_news.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
